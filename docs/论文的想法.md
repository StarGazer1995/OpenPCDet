# 论文的想法

此文档用于记录一些在模型的结构中出现的想法

### Voxelization:

​	1: Calculating the mean value of containing points 

### Devoxelization:

​	1:Collecting data from different channels
​	2:Calculating the probability of a feature exists

### Multi-Resolution Encoding:

​	1: Points from de-voxelization, $p_{ij}$, will find their kth nearest point features, $f_{mn}$, from the lower level by using KNN algorithm.
​	2: These points will be encoded with their neighbor point features, becoming the new features.

​		The feature could be formed as following:

​				$r_i: [p_i, f_j^k[0], p_i-p_j,||p_i-p_j||] or [p_i, f_j^k[0]]$
​				$f_i:[r_i^1,...,r_i^k]$

​				If the f_j is the lowest level:
​	 				$f_j = p_j$
​				not:
​					 $f_j^k[0] = p_j$

### Attention Pooling:

1: Scoring, and output these features

## Updated at 6th,Aug

### Attention Pooling:

According to the paper of the RandLA-Net, when given a point,$p_i$, it's feature would be computed by following equations:
$$
\begin{align}
s_i^k = g(\hat{f}_i,W)\\
\hat{f}_i = \Sigma_{k=1}^{K}(\hat{f}_i^k * s_i^k)
\end{align}
$$
This, however, does not suit our model. Because of the summary, the related points are blended. From this point, we could not recover the distribution of the relative points when given a feature of a point. As an attempt to solve this problem, I propose a solution. Instead of using sum to weight the feature, I argue concatenate the score after the feature. Our Attention Pooling could be written like this:
$$
\begin{align}
s_i = g(\hat{f}_i,W)\\
\hat{f}_i = \hat{f}_i \oplus s_i
\end{align}
$$
Because some relative points have longer distance from the center point, and they are scored lower than the points are closer to the center. I think we could just drop these points. These far points, in some scenarios, are close to one of the other center points, which may be scored higher.